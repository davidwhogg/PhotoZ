\documentclass[12pt]{article}

\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm d}

\begin{document}

\paragraph{To Abade from Hogg:}
There are two projects.  In the first, we try to use exceedingly bad
spectra to improve redshift estimates.  In the second, we infer the
redshift distribution of very faint galaxies using everything we have
got.

\paragraph{generalities:}
Imagine that you have a sample of $M$ galaxies $m$, for each of which
you have taken data $D_m$, which is a set of photometry and possibly
also other information, including perhaps morphology, or even
spectroscopy.  Each $D_m$ is a multi-dimensional vector or list or
blob of measurements, for which you also have a blob of uncertainty
information $I_m$ (this includes a noise model or uncertainty
estimates).

Different galaxies will, in general, have different measured
quantities.  You can think of this in two different ways: In one, you
put an entry in $D_m$ for every measurement you \emph{could} have, but
then put zeros into the inverse variance $1/\sigma^2$ entries in $I_m$
for the measurements you don't, in fact, have.  In the other, each
blob $D_n$ (and each associated $I_n$) is different for every object.
I think this is a pure implementation detail; I will try to make sure
everything (below) works either way.

If you have a standard photometric-redshift scheme, it can return for
you, for each galaxy $m$, what is called a likelihood $p(D_m \given
z_m,\alpha_m,I_m)$.  This is the probability of the data $D_m$ given a
hypothesized redshift $z_m$, and a list or vector or blob of nuisance
parameters $\alpha_m$ describing the SED or the SFH and dust
properties that set the emitted spectrum of the galaxy.  In a standard
kind of analysis, the likelihood is related to some kind of
chi-squared by
\begin{eqnarray}
\ln p(D_m \given z_m, \alpha_m, I_m) &=& C-\frac{1}{2}\,\chi^2
\quad,
\end{eqnarray}
where $C$ is some constant and $\chi^2$ is the inverse-variance
weighted total square residual of the model (at redshift $z_m$ and
given SED parameters $\alpha_m$); it is a function of the data, the
model parameters, and the uncertainty information.

\paragraph{making use of bad spectra}


\paragraph{inferring the redshift distribution}
Imagine your sample of $M$ galaxies is taken from a well-defined set
of faint galaxies for which you would like to know the redshift
distribution.  Imagine that you have very heterogeneous information; a
few have precise spectroscopic redshifts, some have relatively precise
photometric redshifts, and some could be at any of a wide range of
redshifts.

Let's choose some flexible parameterized (or non-parametric, meaning
having many, many parameters) form for the redshift distribution $p(z
\given \theta)$, where $\theta$ is the vector or list of parameters,
possibly large.  Unfortunately, we aren't going to be able to proceed
in a principled manner if we don't \emph{also} choose a parameterized
form for the distribution of SED parameters as a function of redshift
$p(\alpha \given z, \beta)$, where $\beta$ is the blob of parameters
of the conditional distribution of SED parameters.

The way inference proceeds in this case is by treating these
distribution functions as \emph{prior PDFs} on the parameters for the
individual galaxies, but then learning them in the usual way but on
the \emph{whole collecton of galaxies}.  In that inference, the
individual galaxy parameters will be nuisance parameters.  For each
individual object, the posterior probability looks like
\begin{eqnarray}
p(z_m, \alpha_m \given D_m, I_m, \theta, \beta) &=&
  \frac{1}{Z_m}\,p(D_m \given z_m, \alpha_m, I_m)
  \,p(z_m \given \theta)\,p(\alpha_m \given z_m, \beta)
\quad ,
\end{eqnarray}
where $Z_m$ is a normalization factor....DWH?

Compute full marginalized likelihood....

Comment on informative objects....

\end{document}
