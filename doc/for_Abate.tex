% Copyright 2012 David W. Hogg (NYU).
% All rights reserved.

\documentclass[12pt]{article}

\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm d}

\begin{document}

\paragraph{To Abate from Hogg:\\
The redshift distribution of the faintest objects observed by LSST}
We discussed two projects.  In the first, we try to use exceedingly
bad spectra to improve redshift estimates.  In the second, we infer
the redshift distribution of very faint galaxies using everything we
have got.

\paragraph{generalities:}
Imagine that you have a sample of $M$ galaxies $m$, for each of which
you have taken data $D_m$, which is a set of photometry and possibly
also other information, including perhaps morphology, or even
spectroscopy.  Each $D_m$ is a multi-dimensional vector or list or
blob of measurements, for which you also have a blob of uncertainty
information $I_m$ (this includes a noise model or uncertainty
estimates).

Different galaxies will, in general, have different measured
quantities.  You can think of this in two different ways: In one, you
put an entry in $D_m$ for every measurement you \emph{could} have, but
then put zeros into the inverse variance $1/\sigma^2$ entries in $I_m$
for the measurements you don't, in fact, have.  In the other, each
blob $D_n$ (and each associated $I_n$) is different for every object.
I think this is a pure implementation detail; everything (below) works
either way.

If you have a standard photometric-redshift scheme, it can return for
you, for each galaxy $m$, what is called a likelihood $p(D_m \given
z_m,\alpha_m,I_m)$.  This is the probability of the data $D_m$ given a
hypothesized redshift $z_m$, and a list or vector or blob of nuisance
parameters $\alpha_m$ describing the SED or the SFH and dust
properties that set the emitted spectrum of the galaxy.  In a standard
kind of analysis---where the uncertainties are Gaussian---the
likelihood is related to some kind of chi-squared by
\begin{eqnarray}\displaystyle
\ln p(D_m \given z_m, \alpha_m, I_m) &=& C-\frac{1}{2}\,\chi^2
\\
\chi^2 &\equiv& \sum_{n=1}^{N_m} \frac{[D_{mn}-\tilde{D}_{mn}]^2}{\sigma_{mn}^2}
\quad,
\end{eqnarray}
where $C$ is some constant and $\chi^2$ is the inverse-variance
weighted total square residual of the model (at redshift $z_m$ and
given SED parameters $\alpha_m$), we have assumed that the data blob
$D_m$ has $N_m$ components $D_{mn}$ each of which has uncertainty
variance $\sigma_{mn}^2$ and predicted (expected) value
$\tilde{D}_{mn}$ at parameters $(z_m, \alpha_m)$.  That is, the
residuals, $\chi^2$, and the likelihood are all functions of the data,
the model parameters, and the uncertainty information.

If a particular data point $D_{mn}$ in the data blob $D_m$ is a
photometric measurement in some bandpass (filter curve) $R(\lambda)$
then the predicted photometric measurement $\tilde{D}_{mn}$ is
obtained by some kind of integral that looks like
\begin{eqnarray}\displaystyle
\tilde{D}_{mn} &=& \int \frac{L_{\lambda}(\frac{\lambda}{[1+z_m]})}{4\pi\,D_L^2(z_m)}\,R(\lambda)\,\frac{\lambda\,\dd\lambda}{[1+z_m]}
\quad ,
\end{eqnarray}
where $L_{\lambda}(\lambda)$ is a medium-resolution or high-resolution
luminosity density description that is implicitly a function of the
SED parameters $\alpha_m$, $D_L(z_m)$ is the luminosity distance to
redshift $z_m$, and the integral is $\lambda\,\dd\lambda$ for reasons
given elsewhere (arXiv:astro-ph/0210394).

\paragraph{making use of bad spectra:}
In at faint levels or when photometry is low in signal-to-noise or at
certain redshifts or for certain objects, the uncertainties on the
photometric redshifts can be very large.  That statement (``large
uncertainty'') translates to a statement about the likelihood function
$p(D_m \given z_m, \alpha_m, I_m)$; it says that it has substantial
support over a wide range of $z_m$.  This does not usually mean that
the data are uninformative, it just means that they don't say much
about the redshift alone.

However, typically, because some photometric measurements are precise,
those measurements pin the galaxy to a narrow locus in the
$(z_m,\alpha_m)$ hypervolume.  Different SED shapes $\alpha_m$ have
different implications for observed-band colors at different
redshifts.  But \emph{also} different $\alpha_m$ correspond to
different predictions high-resolution (or typical-resolution)
spectroscopy.  That is, at some possible redshifts $z_m$, object $m$
would have to be (given its photometry) an old galaxy, while at others
it would have to be strongly star forming.  That makes predictions for
spectroscopy, of course.  In one case there is a prediction that there
will be no emission lines; in the other there will be, for example,
strong [O\,II] emission in some small, predictable range of
observed-frame wavelengths.

In principle, even a spectrum that would not provide a confident
redshift estimate (for example a spectrum that would only detect a
single, low significance line and no significant continuum) could
distinguish these scenarios.  That is especially true because the
predictions will be very specific about line ratios and observed-frame
locations.  Even a spectrum that detects no lines and no continuum
could be very informative, as it could rule out many star-forming
hypotheses.  And so on.  A related project---where we use
non-detections to make redshift inferences---is our paper on quasar
redshift prediction (arXiv:1105.3975).

One nice thing about the implied project is that it is extremely easy
to test with existing data, at least in principle.  For example,
photometric redshifts can be estimated in SDSS imaging data, and
tested with SDSS (or other) spectra.  The tests can be performed by
``noisifying'' the photometric data (to make the photometric redshifts
uncertain) and noisifying the spectroscopic data (to make the continua
hard to measure and the lines of low significance.

Another nice thing is that it is possible to objectively identify the
best objects for spectroscopic follow-up, in the sense that it will be
possible to estimate in advance, given the likelihood, what range of
$z_m$ could be ruled out with a spectrum in a particular wavelength
range with particular resolution and signal-to-noise.

I have only talked so far about the likelihood; if you really want to
make probabilistic predictions for the spectroscopy, you want to
sample from the posterior PDF for the redshift and SED.  That requires
priors and Bayes's rule.  More on that below.

\paragraph{inferring the redshift distribution:}
Imagine your sample of $M$ galaxies is taken from a well-defined set
of faint galaxies for which you would like to know the redshift
distribution.  Imagine that you have very heterogeneous information; a
few have precise spectroscopic redshifts, some have relatively precise
photometric redshifts, and some could be at any of a wide range of
redshifts.

Let's choose some flexible parameterized (or non-parametric, meaning
having many, many parameters) form for the redshift distribution $p(z
\given \theta)$, where $\theta$ is the vector or list of parameters,
possibly large.  Unfortunately, we aren't going to be able to proceed
in a principled manner if we don't \emph{also} choose a parameterized
form for the distribution of SED parameters as a function of redshift
$p(\alpha \given z, \beta)$, where $\beta$ is the blob of parameters
of the conditional distribution of SED parameters.

The way inference proceeds in this case is by treating these
distribution functions as \emph{prior PDFs} on the parameters for the
individual galaxies, but then learning them in the usual way but on
the \emph{whole collecton of galaxies}.  In that inference, the
individual galaxy parameters will be nuisance parameters.  For each
individual object, the posterior probability looks like
\begin{eqnarray}\displaystyle
p(z_m, \alpha_m \given D_m, I_m, \theta, \beta) &=&
  \frac{1}{Z_m}\,p(D_m \given z_m, \alpha_m, I_m)
  \,p(z_m \given \theta)\,p(\alpha_m \given z_m, \beta)
\quad ,
\end{eqnarray}
where $Z_m$ is a normalization factor.  But instead of making this
posterior probability, we can also make likelihoods for the
hyperparameters $\theta$ and $\beta$ with the individual-galaxy
redshift assignments (and SED inferences) marginalized out:
\begin{eqnarray}\displaystyle
p(D_m \given I_m, \theta, \beta) &=&
  \int p(D_m \given z_m, \alpha_m, I_m)
  \,p(z_m \given \theta)\,p(\alpha_m \given z_m, \beta)\,\dd z_m\,\dd\alpha_m
\quad ,
\end{eqnarray}
where implicitly the integral goes over the full domain of $z_m$ and
the multi-dimensional domain of $\alpha_m$.  This \emph{partially
  marginalized likelihood} is also a \emph{likelihood for the
  hyperparameters}; it is a tool for inferring the parameters of the
redshift and SED distributions.  Each marginalization integral can be
done independently for each object $m$, and then (as long as the data
are independently observed) these partially marginalized likelihoods
can be multiplied to make the full-data likelihood for the
hyperparameters
\begin{eqnarray}\displaystyle
p(D \given I, \theta, \beta) &=&
  \prod_{m=1}^M p(D_m \given I_m, \theta, \beta)
\\
D &\equiv& \left\{D_m\right\}_{m=1}^M
\\
I &\equiv& \left\{I_m\right\}_{m=1}^M
\quad ,
\end{eqnarray}
where $D$ is the full set of all data, and $I$ the full set of all
prior information.  Inference can proceed by maximizing this total
partially marginalized likelihood, or by placing a prior PDF on the
hyperparameters themselves and making a posterior PDF for them.  An
example of this kind of inference is our paper on exoplanet
eccentricities (arXiv:1008.4146).

One nice thing about this approach is that there is no requirement
that all $D_m$ be qualitatively similar; some objects might have
many-band data, some might have only a few measurements; some objects
might have spectroscopy, most won't; non-detections in the UV for some
objects can be very informative; and so on.  Another interesting
aspect of all this is that some particular objects---and you never
know which ones in advance---will be \emph{far more informative} about
the redshift distribution $p(z \given \theta)$ than others.  These
sources can be identified by looking at the dependences of the
inference on the sample; once they have been identified, they mark
themselves for further follow-up.

Along the same lines, once the hyperparameters have been inferred, the
model will make strong predictions for some objects, in the form of
very narrow posterior PDFs for particular object redshifts $z_m$.
Those objects provide excellent opportunities for confirming or
testing the model.

\end{document}
